{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from kmean_clustering import kmean_clustering as kc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n",
      "3.5.3 |Continuum Analytics, Inc.| (default, Mar  6 2017, 11:58:13) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "Numpy\n",
      "1.12.1\n",
      "Pandas\n",
      "0.20.1\n"
     ]
    }
   ],
   "source": [
    "print('Python')\n",
    "print(sys.version)\n",
    "print('Numpy')\n",
    "print(np.__version__)\n",
    "print('Pandas')\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_weight_matrix(weight_matrix):\n",
    "    plt.imshow(weight_matrix)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_data = {}\n",
    "sim_data_names = ['Aggregation',  'Compound',  'D31',  'flame', 'jain',  'pathbased',  'R15',  'spiral']\n",
    "\n",
    "for dn in sim_data_names:\n",
    "    raw_data = pd.read_csv('../data/%s.txt' % dn,sep='\\t',header=None)\n",
    "    label = raw_data.iloc[:,2].values\n",
    "    data   = raw_data.drop(2,axis=1).values\n",
    "    cluster_data[dn] = (data,label)\n",
    "    #print(dn,'#true clusters',cluster_data[dn].iloc[:,2].nunique())\n",
    "    del(raw_data,data,label)\n",
    "    #cluster_data[dn].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#cluster_data = {}\n",
    "\n",
    "### data were downloaded from https://github.com/deric/clustering-benchmark/tree/master/src/main/resources/datasets/real-world\n",
    "### except the leaves data\n",
    "\n",
    "real_data_names = ['letter','wine','glass','thyroid','yeast','iris','leaves','wdbc']\n",
    "\n",
    "## letter 16 + 1\n",
    "raw_data = pd.read_csv('../data/letter.txt',header=None)\n",
    "data  = raw_data.drop(16,axis=1).values\n",
    "label = pd.Categorical(raw_data.iloc[:,16]).codes\n",
    "cluster_data['letter'] = (data,label)\n",
    "del(raw_data,data,label)\n",
    "\n",
    "## wine 1 + 13\n",
    "raw_data = pd.read_csv('../data/wine.txt',header=None)\n",
    "data  = raw_data.drop(0,axis=1).values\n",
    "## scale the data\n",
    "data = MinMaxScaler().fit_transform(X=data)\n",
    "label = pd.Categorical(raw_data.iloc[:,0]).codes\n",
    "cluster_data['wine'] = (data,label)\n",
    "del(raw_data,data,label)\n",
    "\n",
    "## glass 9 + 1\n",
    "raw_data = pd.read_csv('../data/glass.txt',header=None)\n",
    "data  = raw_data.drop(9,axis=1).values\n",
    "label = pd.Categorical(raw_data.iloc[:,9]).codes\n",
    "cluster_data['glass'] = (data,label)\n",
    "del(raw_data,data,label)\n",
    "\n",
    "## thyroid 5 + 1\n",
    "raw_data = pd.read_csv('../data/thyroid.txt',header=None)\n",
    "data  = raw_data.drop(5,axis=1).values\n",
    "## scale the data\n",
    "data = MinMaxScaler().fit_transform(X=data)\n",
    "### merge class 2 and 3 into one class\n",
    "label = pd.Categorical(raw_data.iloc[:,5].replace(3,2)).codes.copy()\n",
    "cluster_data['thyroid'] = (data,label)\n",
    "del(raw_data,data,label)\n",
    "\n",
    "\n",
    "## yeast 1 + 8 + 1\n",
    "raw_data = pd.read_csv('../data/yeast.txt',sep='\\s+',header=None)\n",
    "data  = raw_data.drop([0,9],axis=1).values\n",
    "label = pd.Categorical(raw_data.iloc[:,9]).codes.copy()\n",
    "cluster_data['yeast'] = (data,label)\n",
    "del(raw_data,data,label)\n",
    "\n",
    "## iris 4 + 1\n",
    "raw_data = pd.read_csv('../data/iris.txt',header=None)\n",
    "data  = raw_data.drop(4,axis=1).values\n",
    "## scale the data\n",
    "data = MinMaxScaler().fit_transform(X=data)\n",
    "label = pd.Categorical(raw_data.iloc[:,4]).codes.copy()\n",
    "cluster_data['iris'] = (data,label)\n",
    "del(raw_data,data,label)\n",
    "\n",
    "## wdbc 1 + 1 + 30\n",
    "raw_data = pd.read_csv('../data/wdbc.txt',header=None)\n",
    "data  = raw_data.drop([0,1],axis=1).values\n",
    "## scale the data\n",
    "data = MinMaxScaler().fit_transform(X=data)\n",
    "label = pd.Categorical(raw_data.iloc[:,1]).codes.copy()\n",
    "cluster_data['wdbc'] = (data,label)\n",
    "del(raw_data,data,label)\n",
    "\n",
    "\n",
    "## leaves 1 + 1 + 30\n",
    "raw_data = pd.read_csv('../data/leaves.txt', sep='\\s+',header=None)\n",
    "data  = raw_data.values\n",
    "## scale the data\n",
    "data = MinMaxScaler().fit_transform(X=data)\n",
    "raw_data_label = pd.read_csv('../data/leaves_labels.txt',header=None)\n",
    "label = pd.Categorical(raw_data_label.iloc[:,0]).codes.copy()\n",
    "cluster_data['leaves'] = (data,label)\n",
    "del(raw_data,data,label,raw_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true clusters letter 26 shape (20000, 16)\n",
      "true clusters wine 3 shape (178, 13)\n",
      "true clusters glass 6 shape (214, 9)\n",
      "true clusters thyroid 2 shape (215, 5)\n",
      "true clusters yeast 10 shape (1484, 8)\n",
      "true clusters iris 3 shape (150, 4)\n",
      "true clusters leaves 100 shape (1600, 64)\n",
      "true clusters wdbc 2 shape (569, 30)\n",
      "true clusters Aggregation 7 shape (788, 2)\n",
      "true clusters Compound 6 shape (399, 2)\n",
      "true clusters D31 31 shape (3100, 2)\n",
      "true clusters flame 2 shape (240, 2)\n",
      "true clusters jain 2 shape (373, 2)\n",
      "true clusters pathbased 3 shape (300, 2)\n",
      "true clusters R15 15 shape (600, 2)\n",
      "true clusters spiral 3 shape (312, 2)\n"
     ]
    }
   ],
   "source": [
    "data_names = real_data_names + sim_data_names\n",
    "for dn in data_names:\n",
    "    print('true clusters',dn,len(np.unique(cluster_data[dn][1])),'shape',cluster_data[dn][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile functions-adaptive clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dn = 'wine'\n",
    "(data,labels)  = cluster_data[dn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.08 ms ± 4.34 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "254 µs ± 186 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit scipy.spatial.distance_matrix(data,data)\n",
    "%timeit scipy.spatial.distance.cdist(data, data, 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m1 = scipy.spatial.distance_matrix(data,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m2 = scipy.spatial.distance.cdist(data, data, 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.62683248,  0.57331769, ...,  1.42788461,\n",
       "         1.35195151,  1.52021614],\n",
       "       [ 0.62683248,  0.        ,  0.72090904, ...,  1.33922263,\n",
       "         1.27502377,  1.49688615],\n",
       "       [ 0.57331769,  0.72090904,  0.        , ...,  1.28971502,\n",
       "         1.22263247,  1.34533449],\n",
       "       ..., \n",
       "       [ 1.42788461,  1.33922263,  1.28971502, ...,  0.        ,\n",
       "         0.39924775,  0.60992201],\n",
       "       [ 1.35195151,  1.27502377,  1.22263247, ...,  0.39924775,\n",
       "         0.        ,  0.6145566 ],\n",
       "       [ 1.52021614,  1.49688615,  1.34533449, ...,  0.60992201,\n",
       "         0.6145566 ,  0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.62683248,  0.57331769, ...,  1.42788461,\n",
       "         1.35195151,  1.52021614],\n",
       "       [ 0.62683248,  0.        ,  0.72090904, ...,  1.33922263,\n",
       "         1.27502377,  1.49688615],\n",
       "       [ 0.57331769,  0.72090904,  0.        , ...,  1.28971502,\n",
       "         1.22263247,  1.34533449],\n",
       "       ..., \n",
       "       [ 1.42788461,  1.33922263,  1.28971502, ...,  0.        ,\n",
       "         0.39924775,  0.60992201],\n",
       "       [ 1.35195151,  1.27502377,  1.22263247, ...,  0.39924775,\n",
       "         0.        ,  0.6145566 ],\n",
       "       [ 1.52021614,  1.49688615,  1.34533449, ...,  0.60992201,\n",
       "         0.6145566 ,  0.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilize weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(n_points,n_features) = data.shape\n",
    "distance_matrix = scipy.spatial.distance.cdist(data, data, 'euclidean')\n",
    "weight_matrix = np.zeros(shape=(n_points,n_points))\n",
    "sorted_distance_idx_matrix = np.argsort(distance_matrix,axis=1)\n",
    "sorted_distance_matrix = np.sort(distance_matrix,axis=1)\n",
    "n0 = 2*n_features+2\n",
    "h0 = sorted_distance_matrix[:,n0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.1 ms ± 441 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "46.2 ms ± 170 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "149 µs ± 253 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def f1(h0,n_points,weight_matrix,distance_matrix):\n",
    "    max_h0 = np.reshape([np.maximum(h0[i],h0[j]) for i in range(n_points) for j in range(n_points)],newshape=(n_points,n_points))\n",
    "    weight_matrix = (distance_matrix <= max_h0).astype('int')\n",
    "    return(weight_matrix)\n",
    "\n",
    "def f2(h0,n_points,weight_matrix,distance_matrix):\n",
    "    for i in range(n_points):\n",
    "        for j in range(n_points):\n",
    "            if i==j: weight_matrix[j,i]=1\n",
    "            if i>j:\n",
    "                weight_matrix[i,j] = (distance_matrix[i,j] <= np.maximum(h0[i],h0[j])).astype('int')\n",
    "                weight_matrix[j,i] = weight_matrix[i,j]\n",
    "    return(weight_matrix)\n",
    "\n",
    "def f3(h0,n_points,weight_matrix,distance_matrix):\n",
    "    h0_matrix = np.tile(h0, (n_points, 1))\n",
    "    h0_matrix_T = h0_matrix.T\n",
    "    weight_matrix = (distance_matrix<=np.maximum(h0_matrix,h0_matrix_T)).astype('int')\n",
    "    return(weight_matrix)\n",
    "\n",
    "%timeit f1(h0,n_points,weight_matrix,distance_matrix)\n",
    "%timeit f2(h0,n_points,weight_matrix,distance_matrix)         \n",
    "%timeit f3(h0,n_points,weight_matrix,distance_matrix)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate test statistic T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### for loop version\n",
    "### copy & past the function here for debug purpose\n",
    "\n",
    "import scipy\n",
    "#from scipy.spatial import distance_matrix\n",
    "\n",
    "def adaptive_cluster(data, gap_par = 0.5, n0=None,debug=False):\n",
    "    weight_matrix_history = []\n",
    "    (n_points,n_features) = data.shape\n",
    "    #distance_matrix = scipy.spatial.distance_matrix(data,data)\n",
    "    ## faster version\n",
    "    distance_matrix = scipy.spatial.distance.cdist(data, data, 'euclidean')\n",
    "    #print('distance_matrix.shape',distance_matrix.shape)\n",
    "    weight_matrix = np.zeros(shape=(n_points,n_points))\n",
    "    weight_matrix_history.append((0,weight_matrix.copy()))\n",
    "    #print('weight_matrix.shape',weight_matrix.shape)\n",
    "    #plot_weight_matrix(weight_matrix)\n",
    "    ### sort the distance matrix\n",
    "    sorted_distance_idx_matrix = np.argsort(distance_matrix,axis=1)\n",
    "    sorted_distance_matrix = np.sort(distance_matrix,axis=1)\n",
    "    #print('sorted_distance_matrix.shape',sorted_distance_matrix.shape)    \n",
    "    #print('sorted_distance_idx_matrix.shape',sorted_distance_idx_matrix.shape)\n",
    "    ### number of neighbors\n",
    "    if n0 is None:\n",
    "        n0 = 2*n_features+2\n",
    "    ### h0 is the the radius such that the point has n0 neighbors \n",
    "    h0 = sorted_distance_matrix[:,n0]\n",
    "    #print('h0.shape',h0.shape)\n",
    "    ### max(h0(Xi),h0(Xj))\n",
    "    #max_h0 = np.reshape([np.maximum(h0[i],h0[j]) for i in range(n_points) for j in range(n_points)],newshape=(n_points,n_points))\n",
    "    #print('max_h0.shape',max_h0.shape)\n",
    "    ### weight_matrix\n",
    "    #weight_matrix = (distance_matrix <= max_h0).astype('int')\n",
    "    #print('weight_matrix.shape',weight_matrix.shape)\n",
    "    ### faster version\n",
    "    h0_matrix = np.tile(h0, (n_points, 1))\n",
    "    h0_matrix_T = h0_matrix.T\n",
    "    h0_matrix_max = np.maximum(h0_matrix,h0_matrix_T)\n",
    "    weight_matrix = (distance_matrix<=h0_matrix_max).astype('int')\n",
    "    #plot_weight_matrix(weight_matrix)\n",
    "\n",
    "\n",
    "    #################################################################\n",
    "    ### find h sequence\n",
    "    a = 1.4142135623730951\n",
    "    b = 1.95\n",
    "    #gap_par = -1 \n",
    "    max_distance = np.max(sorted_distance_matrix)\n",
    "    ### h0 is a vector, each data point has n0 neighbors\n",
    "    ### max(h0) makes sure that each data point has at least n0 neighbors\n",
    "    h_array  = np.array([np.max(h0)])\n",
    "    #n_matrix = np.repeat(n0, n_points)\n",
    "    #n_matrix = n_matrix[:,np.newaxis]\n",
    "    k = 0\n",
    "    weight_matrix_history.append((h_array[k],weight_matrix.copy()))\n",
    "    while h_array[k] <= max_distance:\n",
    "        ### upper bound of n(Xi,h_k+1)\n",
    "        ### given radius h_array[k], how many neighbors for each data point\n",
    "        ### -1 removes its self from counting\n",
    "        n_upper = a * np.array([np.sum(sorted_distance_matrix[i,:]<=h_array[k])-1 for i in np.arange(n_points)])\n",
    "        n_upper = (np.floor(n_upper)).astype('int')\n",
    "        ### when h is big, the n_upper may be > n_points\n",
    "        n_upper = np.clip(n_upper, a_min=None,a_max=(n_points-1))\n",
    "        #print(n_upper)\n",
    "        ### n_upper can decide the h_upper\n",
    "        h_upper_by_n_upper = np.min(np.array([sorted_distance_matrix[i,n_upper[i]] for i in np.arange(n_points)]))\n",
    "        ### upper bound of h_k+1\n",
    "        h_upper = b*h_array[k]\n",
    "        ### must satisfy both conditions\n",
    "        min_h_upper = np.minimum(h_upper_by_n_upper,h_upper)\n",
    "        #print(k,min_h_upper)\n",
    "        ### append to the h_array\n",
    "        ### just make sure h is not > max_distance\n",
    "        if min_h_upper <= max_distance:\n",
    "            if  min_h_upper <= h_array[k]: break\n",
    "            #print(k,'h',min_h_upper)\n",
    "            h_array = np.append(h_array,min_h_upper)\n",
    "        k = k + 1\n",
    "\n",
    "    #################################################################    \n",
    "    ### check if those h satisfy the conditions\n",
    "    if debug:\n",
    "        for k in range(1,len(h_array)):  \n",
    "            if h_array[k] <= b*h_array[k-1]:\n",
    "                continue\n",
    "                print('k',k,h_array[k],h_array[k-1],b*h_array[k-1],end=',')\n",
    "                print(h_array[k]/h_array[k-1])\n",
    "            else:\n",
    "                print('h error')\n",
    "        for k in range(1,len(h_array)):\n",
    "            for i in range(n_points):\n",
    "                n1 = np.sum(sorted_distance_matrix[i,:]<=h_array[k-1])-1 \n",
    "                n2 = np.sum(sorted_distance_matrix[i,:]<=h_array[k])-1 \n",
    "                if n2<=a*n1 and n1>=n0 and n2>=n0:\n",
    "                    continue\n",
    "                    print('n',k,n1,n2,a*n1,end=',')\n",
    "                    print(n2/n1)\n",
    "                else:\n",
    "                    print('n error')\n",
    "        \n",
    "    #################################################################\n",
    "    beta_a = (n_features+1.0)/2.0\n",
    "    beta_b = 0.5\n",
    "    beta_function = scipy.special.beta(beta_a,beta_b)\n",
    "\n",
    "    np.seterr(divide='ignore', invalid='ignore')  \n",
    "\n",
    "    for k in range(1,len(h_array)):\n",
    "        print('h_k',h_array[k])\n",
    "        #t_matrix = distance_matrix/h_array[k-1]\n",
    "        #beta_x_matrix = 1.0-(t_matrix**2)/4.0\n",
    "        #incomplete_beta_function_matrix = scipy.special.betainc(beta_a,beta_b,beta_x_matrix)\n",
    "        #q_matrix = incomplete_beta_function_matrix / (2*beta_function-incomplete_beta_function_matrix)\n",
    "        for i in range(n_points):\n",
    "            weight_matrix[i,i] = 1\n",
    "            for j in range(i,n_points):\n",
    "                #if  weight_matrix[i,j] == 1:\n",
    "                #    continue\n",
    "                #if i == j: \n",
    "                #    weight_matrix[i,j] = 1\n",
    "                #    continue\n",
    "                #if i > j:\n",
    "                #    weight_matrix[i,j] = weight_matrix[j,i]\n",
    "                #   continue\n",
    "                if distance_matrix[i,j] <= h_array[k] and h_array[k-1] >= h0[i] and h_array[k-1] >= h0[j]:\n",
    "                    #### caclulate overlap\n",
    "                    N_overlap = np.dot(weight_matrix[i,:],weight_matrix[j,:])\n",
    "                    #### caclulate complement\n",
    "                    #N_complement = np.zeros(shape=(n_points,n_points))\n",
    "                    if k>1:\n",
    "                        ind1 = (distance_matrix[j,:] > h_array[k-1]) + 0.0\n",
    "                        ind2 = (distance_matrix[i,:] > h_array[k-1]) + 0.0\n",
    "                    else:\n",
    "                        ind1 = (distance_matrix[j,:] > h0_matrix_max[i,j]) + 0.0\n",
    "                        ind2 = (distance_matrix[i,:] > h0_matrix_max[i,j]) + 0.0\n",
    "                    N_complement = np.dot(weight_matrix[i,:],ind1) + np.dot(weight_matrix[j,:],ind2)\n",
    "                    #### caclulate union\n",
    "                    N_union = N_overlap + N_complement\n",
    "                    #### theta\n",
    "                    theta = N_overlap / N_union\n",
    "                    #### q\n",
    "                    t = distance_matrix[i,j]/h_array[k-1]\n",
    "                    beta_x = 1.0-(t**2)/4.0\n",
    "                    incomplete_beta_function = scipy.special.betainc(beta_a,beta_b,beta_x)\n",
    "                    q = incomplete_beta_function / (2*beta_function-incomplete_beta_function)\n",
    "                    #q = q_matrix[i,j]\n",
    "                    T1 = N_union\n",
    "                    #### this may raise warnings about log(0) or log(nan)\n",
    "                    #### this is fine, since I used the whole matrix here\n",
    "                    #### some of the points are out of the h(k) radius\n",
    "                    #### we will mask those points in the later step\n",
    "                    T2 = theta*np.log(theta/q)+(1.0-theta)*np.log((1.0-theta)/(1.0-q))\n",
    "                    #### when N_overlap is 0, theta is 0, this leands to T is nan\n",
    "                    #### replace those nan with 0 in T\n",
    "                    #T2 = np.where(theta==0.0,0.0,T2)\n",
    "                    #T2 = np.where(theta==1.0,0.0,T2)\n",
    "                    #T3 = ((theta<=q).astype('int')-(theta>q).astype('int'))\n",
    "                    ### faster version\n",
    "                    if theta<=q:\n",
    "                        T = T1 * T2\n",
    "                    else:\n",
    "                        T = - (T1 * T2)\n",
    "                    #T = T1 * T2 * T3\n",
    "                    ####\n",
    "                    ####\n",
    "                    #weight_matrix[i,j] = (distance_matrix[i,j]<=h_array[k]) * (T<=gap_par) + 0.0\n",
    "                    weight_matrix[i,j] = (T<=gap_par) + 0.0\n",
    "                    #### be careful with those boundary points\n",
    "                    #### theta=0 means no overlap at all\n",
    "                    #### theta=1 means completely overlap\n",
    "                    #### needs special treatment for them\n",
    "                    if theta==0: weight_matrix[i,j] = 0 \n",
    "                    if theta==1: weight_matrix[i,j] = 1\n",
    "                    ####\n",
    "                    weight_matrix[i,j] = weight_matrix[j,i]\n",
    "        weight_matrix_history.append((h_array[k],weight_matrix.copy()))\n",
    "        \n",
    "    ### reset to default\n",
    "    np.seterr(divide='warn', invalid='warn')  \n",
    "    \n",
    "    ### calculate S\n",
    "    S = np.sum(weight_matrix)\n",
    "    \n",
    "    ### extract clusters from weight matrix\n",
    "    labels = (np.zeros(shape=weight_matrix.shape[0]))\n",
    "    labels.fill(np.nan)\n",
    "    cluster_ind = 0\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            if i == j:continue\n",
    "            if weight_matrix[i,j] == 1:\n",
    "                if np.isnan(labels[i]) and np.isnan(labels[j]):\n",
    "                    labels[i] = cluster_ind\n",
    "                    labels[j] = cluster_ind\n",
    "                    cluster_ind = cluster_ind + 1\n",
    "                elif not np.isnan(labels[i]) and np.isnan(labels[j]):\n",
    "                    labels[j] = labels[i]\n",
    "                elif  np.isnan(labels[i]) and not np.isnan(labels[j]):\n",
    "                    labels[i] = labels[j]\n",
    "                elif  not np.isnan(labels[i]) and  not np.isnan(labels[j]):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(i,j,labels[i],labels[j])\n",
    "                    print('cluster assignment error')\n",
    "    ### some points may not belong to any cluster\n",
    "    ### assign those points to the nearest cluster\n",
    "    ### or they can be ignored (by default, those points will have np.nan as labels)\n",
    "    ### thus those points can be considered as outliers\n",
    "    if np.sum(np.isnan(labels))>0:\n",
    "        nan_ind = np.argwhere(np.isnan(labels)).flatten()\n",
    "        for i in nan_ind:\n",
    "            dist = distance_matrix[i,:].copy()\n",
    "            dist[i] = np.max(dist)\n",
    "            nearest_ind = np.argmin(dist)\n",
    "            labels[i] = labels[nearest_ind]\n",
    "            #print(dist)\n",
    "            #print(i,nearest_ind)\n",
    "            \n",
    "    return({\"S\":S,\"weight_matrix\":weight_matrix,\n",
    "            \"cluster_label\":labels,\n",
    "            \"weight_matrix_history\":weight_matrix_history,\n",
    "           })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n",
      "h_k 1.1981658233\n",
      "h_k 1.24993687267\n",
      "h_k 1.2514869379\n"
     ]
    }
   ],
   "source": [
    "dn = 'wine'\n",
    "(data,labels)  = cluster_data[dn]\n",
    "\n",
    "%load_ext line_profiler\n",
    "%lprun -f adaptive_cluster adaptive_cluster(data, gap_par = 0.5, n0=None,debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.24 µs ± 16.1 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "2.39 µs ± 12.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.sum(np.array([0,0,1]) & np.array([1,0,1]))\n",
    "%timeit np.dot(np.array([0,0,1]) , np.array([1,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile functions-k means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  scipy.spatial.distance import pdist,squareform,cdist\n",
    "#from scipy.spatial import distance_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "\n",
    "def k_means(data, n_clusters=3, n_init=20, max_iter=100, kernel=None,\n",
    "            verbose=False,sigma = 1.0,use_kmean_controid=False):\n",
    "    '''\n",
    "    data: a numeric numpy array\n",
    "    n_clusters: number of clusters\n",
    "    n_init: number of different initializations to run kmeans\n",
    "    max_iter: number of max iterations \n",
    "    kernel: \"None\", regular k means; \"gaussian\",  k means with gaussian kernel\n",
    "    verbose: output detailed information\n",
    "    sigma: the sigma parameter in the gaussian kernel \n",
    "    use_kmean_controid: for kenel K means, use the best controids from K means as initialization points.\n",
    "    '''\n",
    "    ### may not be efficient in terms of memory use\n",
    "    ### no need to save whole history\n",
    "    ### get whole hitory for debugging purpose\n",
    "    controid_history = {}\n",
    "    cluster_label_history = {}\n",
    "    sse_history = np.zeros(shape=(n_init,1))\n",
    "    ### start k-means\n",
    "    n_points = data.shape[0]\n",
    "    ### calculate the kernel matrix\n",
    "    if kernel == 'gaussian':\n",
    "        ### 'sqeuclidean': squared Euclidean distance\n",
    "        kernel_matrix = np.exp(-0.5/(sigma**2)*squareform(pdist(data,'sqeuclidean')))\n",
    "    ### repeat k-means n_init times \n",
    "    ### return the best one \n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    for i_init in range(n_init):\n",
    "        if verbose: print('Random seed',i_init)\n",
    "        #### set random seed\n",
    "        np.random.seed(i_init)\n",
    "        #### generate initial cluster labels\n",
    "        cluster_labels = np.random.choice(range(n_clusters),size=n_points, replace=True)\n",
    "        #### generate initial centroids\n",
    "        #### randomly choose n_clusters points from the data as centroids\n",
    "        if use_kmean_controid:\n",
    "            #### run one K means\n",
    "            print('Use best K means centroid')\n",
    "            km_result = k_means(data, n_clusters, n_init=20, max_iter=100, kernel=None)\n",
    "            centroids = km_result['best_controids']\n",
    "        else:\n",
    "            #### randomly choose n_clusters points from the data as centroids\n",
    "            centroids = data[np.random.choice(np.arange(n_points), n_clusters, replace=False),:]\n",
    "        for i_iter in range(max_iter):\n",
    "            if verbose: print('Iteration',i_iter,end=', ')\n",
    "            distance_to_centroids = np.zeros(shape=(data.shape[0],n_clusters))\n",
    "            ######\n",
    "            if kernel is None:\n",
    "                distance_to_centroids = scipy.spatial.distance.cdist(data, centroids, 'euclidean')\n",
    "            ######\n",
    "            elif kernel == 'gaussian':\n",
    "                dist1 = np.diag(kernel_matrix)\n",
    "                cluster_ind_matrix = np.zeros(shape=(data.shape[0],n_clusters))\n",
    "                for i_centroid in range(n_clusters):\n",
    "                    cluster_ind_matrix[:,i_centroid] = (cluster_labels == i_centroid) + 0.0\n",
    "                    kth_cluster_ind = (cluster_labels == i_centroid) + 0.0\n",
    "                    kth_cluster_matrix = np.outer(kth_cluster_ind,kth_cluster_ind)\n",
    "                    \n",
    "                    dist2 = 2.0*np.sum(np.tile(kth_cluster_ind,(n_points,1))*kernel_matrix,axis=1)/np.sum(kth_cluster_ind)\n",
    "                    dist3 = np.sum(kth_cluster_matrix*kernel_matrix)/np.sum(kth_cluster_matrix)\n",
    "                    #print(dist1.shape,dist2.shape,dist3.shape,)\n",
    "                    ### ord=2 is L2 distance\n",
    "                    ### axis=1 is to calculate norm along columns\n",
    "                    distance_to_centroids[:,i_centroid] = dist1-dist2+dist3\n",
    "                    #break\n",
    "            else:\n",
    "                sys.exit('Kernel parameter is not correct!')\n",
    "            #print(distance_to_centroids)\n",
    "            ### assign the cluster labels\n",
    "            cluster_labels = np.argmin(distance_to_centroids,axis=1)\n",
    "            sse = np.sum((np.min(distance_to_centroids,axis=1))**2)\n",
    "            if verbose: print('SSE',sse)\n",
    "            ### re-calculate centroids\n",
    "            previous_centroids = centroids\n",
    "            centroids = np.array([data[cluster_labels == i_centroid].mean(axis = 0) for i_centroid in range(n_clusters)])\n",
    "            ### if centroids don't change\n",
    "            ### stop the iteration\n",
    "            if np.all(previous_centroids == centroids):\n",
    "                if verbose: print('Centroids do not change',i_iter)\n",
    "                break\n",
    "            #break\n",
    "        controid_history[i_init] = centroids\n",
    "        cluster_label_history[i_init] = cluster_labels\n",
    "        sse_history[i_init] = sse\n",
    "        #break\n",
    "    np.seterr(divide='warn', invalid='warn')\n",
    "    ### find the best initializations\n",
    "    best_iter = np.argmin(sse_history)\n",
    "    best_sse = sse_history[best_iter]\n",
    "    best_controids = controid_history[best_iter]\n",
    "    best_cluster_label = cluster_label_history[best_iter]\n",
    "    \n",
    "    return {'best_iter':best_iter,\n",
    "            'best_sse':best_sse,\n",
    "            'best_controids':best_controids,\n",
    "            'best_cluster_label':best_cluster_label,\n",
    "            'controid_history':controid_history,\n",
    "            'cluster_label_history':cluster_label_history,\n",
    "            'sse_history':sse_history,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n",
      "Use best K means centroid\n"
     ]
    }
   ],
   "source": [
    "dn = 'Aggregation'\n",
    "(data,labels)  = cluster_data[dn]\n",
    "\n",
    "%load_ext line_profiler\n",
    "%lprun -f k_means k_means(data, n_clusters=3, n_init=1, max_iter=100, kernel='gaussian',verbose=False,sigma = 1.0,use_kmean_controid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.03 µs ± 22 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "6.63 µs ± 120 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0,1,0,0,0,1,1,1,1,0,0,0,0,1])\n",
    "l = len(a)\n",
    "np.outer(a,a)\n",
    "def f(a,l):\n",
    "    a_matrix = np.tile(a,(l,1))\n",
    "    return a_matrix * a_matrix.T\n",
    "    \n",
    "%timeit np.outer(a,a)\n",
    "%timeit f(a,l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "548px",
    "left": "0px",
    "right": "1089px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
